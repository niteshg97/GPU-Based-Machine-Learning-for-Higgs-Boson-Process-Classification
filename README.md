# âš›ï¸ GPU-Based Machine Learning for Higgs Boson Process Classification

## ğŸ§  Introduction
The discovery of the **Higgs boson particle** at CERN in 2012 marked a monumental achievement in modern physics, confirming the existence of the **Higgs field** â€” a cornerstone of the Standard Model.  
Detecting such elusive particles requires **advanced computing**, sophisticated **machine learning (ML)**, and global collaboration ğŸŒ.

This repository implements a **GPU-accelerated machine learning pipeline** for classifying **signal processes** (Higgs-producing) vs **background processes** (non-Higgs).  
It leverages the **RAPIDS** framework for GPU-parallel data processing and **Google Colab (NVIDIA T4 GPU)** for accelerated model training, evaluation, and testing. âš¡

---

## ğŸ§© Background

### ğŸ§¬ Machine Learning in High-Energy Physics
ML has become an essential tool in **particle physics**, traditionally applied to processed data from reconstruction algorithms.  
Todayâ€™s approaches enable **direct analysis of raw detector data**, helping with:
- **Event selection**
- **Event classification**
- **Background suppression**

These innovations allow physicists to separate meaningful *signal* events from enormous *background* noise.

---

### âš™ï¸ Computational Demands
High-energy physics experiments â€” like those at the **Large Hadron Collider (LHC)** â€” generate **tens of terabytes per second** of raw data.  
The **High Luminosity LHC (HL-LHC)** will produce up to **15Ã— more data**.  
Traditional CPUs struggle with this scale, motivating the use of **GPUs** for their massive parallelism in matrix operations and data transformations ğŸš€.

---

### ğŸ¯ Task Specification
In collider experiments, **signal events** correspond to Higgs boson decay, while **background events** come from other particles.  
Using **Monte Carloâ€“simulated data**, this project applies machine learning to classify events as either **signal (1)** or **background (0)**.

---

## ğŸ§± Implementation Steps

### ğŸ—‚ï¸ Data
- **Dataset:** [UCI HIGGS Dataset](https://archive.ics.uci.edu/ml/datasets/HIGGS)  
- **Size:** ~11 million instances generated via Monte Carlo simulations.  
- **Features:** 29 total â€” first column = binary label (1 = signal, 0 = background), 28 physics-based features.  

---

### ğŸ” Exploratory Data Analysis (EDA)
Key analyses performed:
- Checked for **missing values** and **class imbalance**.
- Visualized **feature correlations** to identify redundancy.
- Identified and managed **outliers** using the **3Ã—IQR rule** (less aggressive, preserving more data).  
- Noted strong correlation pairs like `m_wbb` â†” `m_wwbb` and `m_jj` â†” `m_jjj`.

âœ… **Feature reduction:** Removed `m_wbb` and `m_jjj` to prevent redundancy.

---

### ğŸ§¹ Data Preprocessing
- **Removed correlated features** (to avoid multicollinearity).  
- **Handled outliers** (3Ã—IQR trimming).  
- **Addressed class imbalance** using **SMOTE (Synthetic Minority Over-sampling Technique)**.  
- **Normalized** using `cuML`â€™s GPU-based standard scaler for better convergence and accuracy.  
- Saved processed datasets for reuse and rapid model training.

---

## ğŸ¤– Models
Three machine learning models were trained and compared:

| ğŸ”¢ Model | âš™ï¸ Description | ğŸš€ Accelerator |
|:--|:--|:--|
| **Logistic Regression** | Linear baseline classifier | GPU/CPU |
| **Random Forest** | Ensemble model with bootstrapped decision trees | GPU/CPU |
| **XGBoost** | Gradient-boosted trees (best-performing) | GPU (`gpu_hist`) |

---

## ğŸ§ª Experiments
### ğŸ§® Grid Search & Tuning
A **grid search** was used for hyperparameter tuning to achieve optimal performance.

- **Logistic Regression:** Tested on CPU & GPU â€” similar accuracy, but **GPU 3Ã— faster** â©.  
- **Random Forest:** Tuned `n_estimators` and `max_depth`; balanced accuracy and computational load.  
- **XGBoost:** Trained with and without normalization â€” **normalization significantly improved stability and AUC**.

### ğŸ§­ Principal Component Analysis (PCA)
- Applied **PCA** to explore dimensionality reduction.  
- Re-trained models on reduced feature sets.  
- PCA preserved ~95% variance with 18 components â€” slight drop in accuracy but faster training time.  
- **XGBoost maintained top performance even with reduced dimensions.**

---

## ğŸ§ª Testing and Model Selection
After multiple experiments, **Logistic Regression** was dropped due to underperformance.

| ğŸ§  Model | Accuracy | Precision | Recall | Specificity | F1-Score | AUC | Training Time (s) |
|:--------:|:---------:|:----------:|:----------:|:----------:|:--------:|:----:|:----------------:|
| **Logistic Regression** | 0.61 | 0.61 | 0.61 | 0.62 | 0.69 | 0.73 | 52 |
| **Random Forest** | 0.73 | 0.74 | 0.72 | 0.73 | 0.73 | 0.80 | 176 |
| **ğŸŒŸ XGBoost (Selected)** | **0.74** | **0.75** | **0.73** | **0.74** | **0.74** | **0.82** | **132** |

âœ… **XGBoost outperformed all models**, particularly in **precision and AUC**, making it the most suitable model for Higgs boson process classification.

ğŸ“¦ The trained XGBoost model is saved in:


------

## ğŸ§­ Conclusion
This project demonstrates the effectiveness of **GPU-based ML** in high-energy physics for **Higgs boson process classification**.

**Key achievements:**
- âš¡ **Accelerated training** with Google Colab (T4 GPU).  
- ğŸ“Š Efficient handling of an **11M-row dataset** using RAPIDS-based and GPU-enhanced libraries.  
- ğŸ§© Rigorous preprocessing pipeline (outlier handling, normalization, feature reduction).  
- ğŸ† **XGBoost achieved the best accuracy (74%) and AUC (0.82)** â€” recommended model for deployment.  

---

## âš™ï¸ Tools & Libraries
- **Python 3.10**
- **Google Colab (NVIDIA T4 GPU)**
- **RAPIDS cuML/cuDF**
- **XGBoost** (GPU `gpu_hist` mode)
- **scikit-learn**
- **pandas**, **matplotlib**, **seaborn**
- **imbalanced-learn (SMOTE)**

---

## ğŸ“š References  
1. UCI Machine Learning Repository â€” [HIGGS Dataset](https://archive.ics.uci.edu/ml/datasets/HIGGS).  
2. RAPIDS Framework â€” [https://rapids.ai](https://rapids.ai).  
3. XGBoost Documentation â€” [https://xgboost.readthedocs.io](https://xgboost.readthedocs.io).  
4. CERN Open Data Portal â€” [https://opendata.cern.ch](https://opendata.cern.ch).  

---

## ğŸ Summary
> **Final Recommendation:**  
> Use **XGBoost (GPU-accelerated)** as the primary classifier for Higgs boson process discrimination.  
> Achieved **74% accuracy** and **AUC = 0.82** on the test dataset â€” with superior inference speed and stability on GPU compared to CPU models. âš¡  

**â€œGPU computing has transformed theoretical physics from hours to minutes â€” accelerating discovery itself.â€**

